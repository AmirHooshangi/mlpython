Philosophy:

- should be simple to use and intuitive
- it's the user's job to use it correctly though
- should be flexible
- should rely more on conventions than rules
- once a learner is trained, should be easy to get a functional system out of it

How to present:
- first way of using mlpython: as a library of learners, which take as inputs 
  (at training or test time) iterators over examples (i.e. forget about datasets and mlproblems)
- second way of using mlpython: as a way to structure datasets and experiments 
  for machine learning research

TODO:
- comment modules and packages
- write some templates for the different classes
- generate documentation
- make sure comments in library are uniformly formatted and complete
- add tools for generating/viewing results (k-fold, early-stopping, save results, viewing results in tables or plots)
- figure out a way of having test suites
- write down tutorial
- figure out a way to include other people's software (add a directory in learners, 
  which give all the info/code for downloading other people's software, installing it and
  incorporating it to the library)
- write a couple of datasets modules (libsvm datasets, mnist dataset, norb, caltech, ICML2007, etc.)
- would be nice if the library gave a good result on caltech!

docstrings:

- docstring should give
  * the Required metadata
  * the Defined metadata (if applicable)
  * the options (hyper-parameters) of the object
  * one or more papers for references, if applicable
- if any of the aforementioned items are non applicable
  or if there aren't any of those items (e.g. no required metadata),
  one can either not put the item or write e.g. 'Required metadata: None' on
  the same line
- the format for writing the above items is

  NAME_OF_ITEM:
  - 'ITEM1'[: possible documentation for ITEM1, giving default value and possibly going over many lines]
  - 'ITEM2'[: idem]

- in general, add docstrings is strongly encouraged, but the code should also work
  without it (we want the library to be easy to use)
- might want to code a script which takes a .py file and says whether its items
  follow the right format
- should a name for the concept here refered to as "aforementied items"...
- question: should information about the expected number of fields in the processed 
  dataset be given? HUGO: well, can't really, since a learner might be applicable
  on examples with or without weights (hence for varying numbers of fields)

datasets

- should contain python modules, each defining two function: read(dir_path) and obtain(dir_path)
- obtain(dir_path) is the function to call to obtain the data. In general, it should
  download the data to the given dir_path. If that is not possible, then it should
  at least print details on where/how to obtain the data
- read(dir_path) is the function to call get the data, once it has been obtained. 
  dir_path should be the same as the dir_path used by obtain(dir_path). If necessary
  (which is not uncommon), read can have other options, with default values.
  It can give all the data, a train/valid/test split, a k-fold split, etc.
- should consider adding a 'get_mlproblem(problem)', which would create for you
  some possible problems for a given dataset. Need to figure out a simple
  syntax and use for this function however... The idea would be that it, for
  a give class of problem (classification, regression, density, etc.) it would
  spit out an mlproblem that can be used by the type of learner associated to the
  mlproblem. This will make it possible to automate the evaluation of algorithms.
  

mlproblems

- they are iterators with a metadata field and a "apply_on" and "setup" method
- they are not necessary for using learners, but will make it possible to 
  automate the evaluate of learning algorithms on a vast variety of problems
- they might receive data in some weird format, but they should "output"
  data in a (input, target) pair 
  HUGO: I think we could/should constrain the input to be an iterator
  HUGO: not sure output is supposed to be (input, target) pairs. Maybe we want to have
        weighted datasets! Maybe there isn't an input (unsup. learning)!
	The output "garanty" is that the output is... an MLProblem (with apply_on and setup)
- an input is what can be used by learner.use() 
- __init__ should be very fast, so that apply_on is also fast (it's important, as
  we may whish to apply on just one example, sequentially)
- if dataset set is not a stream, should define __len__ also
  HUGO: is data is large, can define 'length' in metadata
- IMPORTANT: the data and metadata should not be modified, after an mlproblem is constructed!
             Otherwise, weird behaviors might be observed.
	     Example: the metadata 'length' (used to store the length of the dataset) is not
	              updated based on the actual length of data.
- question: what about mlproblems that need to be solved (i.e. datasets for which the target
  is unkown). Should mlproblems be able to handle missing values?
- question: mlproblems can be chained. However, you need to remember to put the metadata
            field of the input mlproblems as a field in the constructor for the
            new mlproblem (e.g. A = mlpbA(data=data,metadata=metadata), B = mlpbB(data=A,metadata=A.metadata)
            Is this too complicated? Should an mlproblem look for metadata in the data field
            (though that would be quite strange)?
            Maybe just write a couple of shorthand functions, that would do that for you?

metadata

- metadata are there to pass around information between the different
  steps, so that they aren't recomputed
- question: how to deal with metadata used by super classes? should the class
            mention those as well? Con: requires reading the upper class and repeats information.
- question: metadata are shared accross the pipeline of preprocs (pointers). 
  Is that a good thing? Pro: saves memory. Con: looking back at metadata
  might give weird behavior.
  HUGO: metadata are dictionaries. So we will copy the key:value pairs,
  but not the values themselves, which will be shared (seems like
  a reasonable way of having some of the pro but avoiding the con)
- the user of the library shouldn't have to be conserned with metadata
  in the general-use case, but might want to play with them
  if he/she faces a somewhat weird problem.
- question: should mlproblems that are the same have the same metadata? 
  HUGO: only the metadata that are specific to the task (like str to class
  mapping)
- should make a list of conventions for the keywords that should be used
  for the different concepts, in the metadata dictionary
- NO!!: some learners will try to figure out the value of those concepts from
  the mlproblem directly
- any mlproblem or learner (or object in general) that requires metadata
  should explicitly say which ones in the docstring. If it
  defines metadata, it should also say so (and which)
- might want to consider defining functions that would look into a 
  docstring and would output the required and defined metadata.
-question: why do we need the setup() function? HUGO: because we only apply it on a training set,
	   not on a test set

preprocs

- need to distinguish preprocessings on single examples with preprocessings 
  on a whole dataset (e.g., concatenating, etc.)

learners

- should state which metadata they are expecting
- forget only works when the learner has been trained at least once
  (this is because part of forget() depends on dataset specific parameters).
  forget is also the only official way of seting self.stage=0 by the user,
  otherwise anything can happen!
- the use() function takes an mlproblem and outputs an iterator with the output
  of the learner (the target of the mlproblem could be missing)
- the test() function gives the output AND the error
- should Learner superclass, use() and test() should be implemented, based
  on the some smaller methods compute_output and compute_error_from_output, which
  work on an input and an example respectively
  * con: to avoid memory creation, you need to know what is the size of the output
         and the number of costs, which is learner specific. Don't want to have to
	 write methods that say how many outputs and whatever...
- question: what do I do about the "stage"? Should I standardize its use? 
            con: not all models have this concept (some are trained in one pass)
	    HUGO: might want to standardize it only in an "online" subclass of
	          algorithms?
- train is the first method to call. Other methods are not expected to
  work before train is called!

- could create a hierarchy of learners, based on what test() does
  and on what metadata are required. 
  For example, for classifiers, it is expected that the first element 
  of the output (computed by use()) is a class id, and test() compares
  it to the label to compute the test error. A classifier also
  requires class_to_id as a metadata.

creating learner pipelines
- we want to have code that does this separately (as opposed to have train or use output
  a dataset), to deal separately with options like appending or replacing, etc.

mathutils

- put there whatever Numpy and Scipy aren't doing for me
- put also utility functions that calling Numpy/Scipy, but in a more convenient way
- at some point, should decide where the numpy checks are being done: in the c code or in 
  the python caller? Now, it's split between both...
- should have methods for creating vectors and matrices of floats and integers, in order 
  to control the size of those types!
- WATCHOUT: weave.inline with type_converters = scipy.weave.converters.blitz doesn't seem 
  to work with fortran arrays! The part of the code that converts to blitz arrays doesn't 
  look at whether the data is row major or column major!
- make sure that math library works on different machines (might be issues with sizes 
  of double and int)


combining everything

- should have a some code that taks all the necessary components of an experiment and runs it
- such a function would make it possible to control and uniformize the sort of output (print) 
  we can expect in an experiment

collecting and viewing results

- should just write results in a text file, with \t separated fields
- then, use a combination of "less" and "sort" to view results
- might want to code something that format each column to fit in a given 
  number of characters, so that the output is nicer

libsvm

- the sparse version of the data representation currently uses 2 numpy arrays.
  libsvm uses a dict mapping. Maybe I should switch?
- TODO: implement "precomputed kernel" option (see libsvm documentation... it would
        be pretty simple to do...)
