.. _tutorial:

Tutorial: How to use MLPython
=============================

.. contents::

Datasets
--------

MLPython comes with support for several datasets (see the
:ref:`datasets` library documentation for a list). To use a supported
dataset, module ``mlpython.datasets.store`` should be used. It
provides procedures for downloading datasets and creating machine
learning problems from them.

First, the dataset must be downloaded (and so only once). To download 
dataset with name ``dataset_name``, simply do the following: ::

   import mlpython.datasets.store as dataset_store
   dataset_store.download(dataset_name)

Then, ``mlpython.datasets.store`` provides several functions that will
create a specific machine learning problem (classification, density
estimation, etc.)  from a downloaded dataset. These functions all
return a training, validation and test split. Here are a few examples
of using such functions (see the :ref:`datasets` library documentation 
for more such functions): ::

   # Generate a classification problem
   trainset,validset,testset = dataset_store.get_classification_problem(dataset_name)
   
   # Generate a density estimation problem
   trainset,validset,testset = dataset_store.get_density_problem(dataset_name)

   # Generate a multilabel classification problem
   trainset,validset,testset = dataset_store.get_multilabel_problem(dataset_name)

Not all datasets can be converted to all different problems. Module
``mlpython.datasets.store`` also contains string lists enumerating the
datasets supported for each machine learning problem (see variables
with name ending in ``names``,
e.g. ``mlpython.datasets.store.classification_names``).


By default, datasets will be downloaded in the directory given by
environment variable MLPYTHON_DATASET_REPO, more specifically in a
subdirectory with the same name as the dataset's. If
MLPYTHON_DATASET_REPO isn't defined, or if you wish to specify some
other path for where to download the dataset, a different path can be
specified through argument ``dataset_dir``. The same is true for
the functions that generate the machine learning problems.

Module ``mlpython.datasets.store`` provides other useful functions,
such as functions to generate a K-fold cross-validation experiment or
a semi-supervised learning experiment. For more information see the
:ref:`datasets` library documentation.

Processed datasets: MLProblems
------------------------------

For a dataset to be fed to a Learner, it must correspond to an MLProblem. 

MLProblem objects are simply iterators with some extra properties.
Hence, from an MLProblem, examples can be obtained by iterating over
the MLProblem. 

MLProblem objects also contain metadata, i.e. "data about the
data". For instance, the metadata could contain information about the
size of the input or the set of all possible values for the
target. The metadata (field ``metadata`` of an MLProblem) is
represented by a dictionary mapping strings to arbitrary objects.  

If you always use ``mlpython.datasets.store`` to generate MLProblems,
you will probably never need to create MLProblems from scratch
yourself, since all datasets returned by this module are already
MLProblems. However, if you are facing an unusual learning paradigm or
wish to use a dataset that is not currently supported by MLPython, you will
need to know how MLProblems work. 

Here are instructions for how to manipulate MLProblems.
To create an MLProblem, simply give the raw data over which
iterating should be done and the dictionary containing the
metadata: ::

   >>> from mlpython.mlproblems.generic import MLProblem
   >>> import numpy as np
   >>>
   >>> data = np.arange(30).reshape((10,3)) 
   >>> metadata = {'input_size':3}
   >>> mlpb = MLProblem(data,metadata)
   >>> for example in mlpb:
   ...     print example
   ...
   [0 1 2]
   [3 4 5]
   [6 7 8]
   [ 9 10 11]
   [12 13 14]
   [15 16 17]
   [18 19 20]
   [21 22 23]
   [24 25 26]
   [27 28 29]


Each Learner will require a specific structure within each example. For instance,
a supervised learning algorithm will require that examples decompose into two 
parts, an input and a target::

   >>> data = [ (input,np.sum(input)) for input in np.arange(30).reshape((10,3))]
   >>> metadata = {'input_size':3}
   >>> mlpb = MLProblem(data,metadata)
   >>> for input,target in mlpb:
   ...     print input,target
   ...
   [0 1 2] 3
   [3 4 5] 12
   [6 7 8] 21
   [ 9 10 11] 30
   [12 13 14] 39
   [15 16 17] 48
   [18 19 20] 57
   [21 22 23] 66
   [24 25 26] 75
   [27 28 29] 84

Each Learner object will expect a certain structure within the
example, and it is the job of MLProblem to be compatible with the
Learner's expected example structure.

Some MLProblem objects might require that specific metadata keys be
given and might even add some more. Here's an example to illustrate
this concept. Imagine we wish to create a training set for a
classification problem. To do this, we create a ClassificationProblem
object, which requires the metadata ``'targets'`` that corresponds to
the set of values that the target can take.  Moreover, after having
created this new MLProblem, its metadata will now contain a key
``'class_to_id'``. This key will be associated to a dictionary that
maps target symbols to class IDs: ::

   >>> from mlpython.mlproblems.classification import ClassificationProblem
   >>> data = [ (input,str(input<5)) for input in range(10) ]
   >>> metadata = {'targets':set(['False','True'])}
   >>> trainset = ClassificationProblem(data,metadata)
   >>> print trainset.metadata['class_to_id']
   {'False': 1, 'True': 0}

Learners will also expect different kinds of metadata. The user should
look at the MLProblem and Learner class docstrings in order to figure
out which metadata are expected by these objects.

Once the training set has been processed as desired, then
the same processing should be applied to the other
datasets, such as the test set, as follows: ::

   >>> test_data = [ (input,str(input<5)) for input in [-2,-1,10,11] ]
   >>> test_metadata = {'targets':set(['False','True'])}
   >>> testset = trainset.apply_on(test_data,test_metadata)
   >>> print testset.metadata['class_to_id']
   {'False': 1, 'True': 0}

This ensures that all datasets are coherent and share the relevant
metadata, such as the ``class_to_id`` mapping for classification
problems.  

MLProblems can also be combined. For instance, to obtain
a subset of the examples of a classification problem,
a ClassificationProblem object can be fed to a SubsetProblem::

   >>> from mlpython.mlproblems.generic import SubsetProblem
   >>> subsetpb = SubsetProblem(trainset,subset=set(range(0,10,2)))
   >>> for example in subsetpb:
   ...     print example
   ... 
   (0, 0)
   (2, 0)
   (4, 0)
   (6, 1)
   (8, 1)
   >>> print subsetpb.metadata
   {'targets': set(['True', 'False']), 'class_to_id': {'False': 1, 'True': 0}}

The new MLProblem will inherit the metadata from the previous
MLProblem. Additional metadata can also be given explicitly in the
constructor, as before. If there is overlap between the keys of the
previous MLProblem and the metadata explicitly given to the
constructor, the later will have priority.

Finally, an MLProblem has a length, which can be obtained by
simply calling function ``len()`` on the object: ::

   >>> len(subsetpb)
   5

Learning algorithms: Learners
-----------------------------

Learning algorithms are implemented as Learner objects. Other than a
constructor (which will take the value of the Learner's hyper-parameters), 
Learners all have four basic methods:

  
* ``train(self, trainset)``:  Runs the learning algorithm on the MLProblem ``trainset``. 
  			      Calling ``train`` a second time should not do anything, unless 
			      some of the hyper-parameters have changed (e.g. the training stopping
			      criteria) or ``forget``Â has been called.
* ``forget(self)``:           Resets the Learner to its initial state.
* ``use(self, dataset)``:     Computes and returns the output of the Learner for
      	      		      MLProblem ``dataset``. The method should return an iterator over these
			      outputs (e.g. a list).
* ``test(self, dataset)``:    Computes and returns the outputs of the Learner as well as the cost of 
                              those outputs for MLProblem ``dataset``. The method should return a 
			      pair of two iterators, the first being over the outputs and the 
			      second over the costs.
  
Learner objects are separated in different module, for different types
of problems (classification, density estimation, etc.). See the
:ref:`learners` library documentation for more information.

My first experiment
-------------------

* Maybe add an experiment on MNIST that gets 1.2%?
* design some templates (either put in documentation only or have them in the code somewhere)

Other tools
-----------

Third-party Learners
--------------------

Extra gift: some useful scripts
-------------------------------

Before you leave
----------------

add useful links for ml in python, in general

  * profiling, e.g. http://packages.python.org/line_profiler/
  * C programming within Numpy

